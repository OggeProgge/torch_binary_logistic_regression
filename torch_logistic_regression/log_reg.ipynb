{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy  # NOTE: install v1.26.4\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_torch_f32 = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(in_features=input_dim, out_features=output_dim, bias=True)\n",
    "        self.metrics = {'accuracy': torchmetrics.Accuracy(task=\"binary\")}\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x).squeeze(-1)  # logits; squeeze() makes [x, 1] -> [x] i.e., just formatting \n",
    "\n",
    "\n",
    "    def reg_loss(self, l1: float) -> torch.Tensor:\n",
    "        \n",
    "        loss = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        if l1 > 0.0:\n",
    "            loss += l1 * torch.sum(abs(self.linear.weight)) \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # inference\n",
    "    def compute_prob(self, x) -> numpy.ndarray:\n",
    "        self.eval()\n",
    "\n",
    "        if isinstance(x, numpy.ndarray):\n",
    "            x = torch.from_numpy(x).float()\n",
    "\n",
    "        elif not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Instance must have type: 1) numpy.ndarray or 2) torch.Tensor\")\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            logits = self(x.to(self.device))\n",
    "            \n",
    "        return torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x, cl_threshold: float = 0.5) -> numpy.ndarray:\n",
    "\n",
    "        return (self.compute_prob(x) > cl_threshold).astype(int)\n",
    "    \n",
    "\n",
    "    # test\n",
    "    def test(self, dataloader: torch.utils.data.DataLoader, loss_fun, metrics=None):\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = []\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for X, y in dataloader:\n",
    "                \n",
    "                X, y = X.to(self.device, dtype=dtype_torch_f32), y.to(self.device, dtype=dtype_torch_f32)\n",
    "                \n",
    "                prediction = self(X)\n",
    "                \n",
    "                loss += loss_fun(prediction, y).item() * len(X)\n",
    "                \n",
    "                for m in metrics:\n",
    "                    m.update(torch.sigmoid(prediction), y.int())        \n",
    "\n",
    "        return loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "    # training\n",
    "    def train_epoch(self, dataloader: torch.utils.data.DataLoader, optim: torch.optim.Optimizer, loss_fun: callable, l1: float, l2: float):\n",
    "\n",
    "        # train mode\n",
    "        self.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(self.device, dtype=dtype_torch_f32), y.to(self.device, dtype=dtype_torch_f32)    \n",
    "\n",
    "            # reset gradients\n",
    "            optim.zero_grad()                      \n",
    "\n",
    "            # prediction error\n",
    "            loss  = loss_fun(self(X), y)\n",
    "            loss += self.reg_loss(l1)\n",
    "\n",
    "            # training loss\n",
    "            train_loss += loss.item() * len(X)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        return train_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "    def train_model(self, \n",
    "                    train_dataloader: torch.utils.data.DataLoader, \n",
    "                    validation_dataloader: torch.utils.data.DataLoader,\n",
    "                    learning_rate: float, \n",
    "                    l1: float = 0.0,\n",
    "                    l2: float = 0.0,\n",
    "                    momentum: float = 0.9,\n",
    "                    loss_fun: callable = torch.nn.BCEWithLogitsLoss(),\n",
    "                    optimizer: str = \"Adam\",\n",
    "                    num_epochs: int = 100):\n",
    "\n",
    "            train_losses = []\n",
    "            validation_losses = []\n",
    "            \n",
    "            # Initialize dictionary to store results\n",
    "            metrics_on_validation_data = {k: [] for k in ['accuracy', 'precision', 'prauc', 'recall', 'f1', 'auroc', 'specificity', 'mcc', 'logauc']}\n",
    "\n",
    "            # Group metrics into a ModuleList or a standard list for easier management\n",
    "            # Note: torchmetrics.MetricCollection is designed exactly for this purpose!\n",
    "            val_metrics = torchmetrics.MetricCollection(\n",
    "                {\n",
    "                    'accuracy':    torchmetrics.Accuracy(task=\"binary\"),\n",
    "                    'precision':   torchmetrics.Precision(task=\"binary\"),\n",
    "                    'prauc':       torchmetrics.AveragePrecision(task=\"binary\"),\n",
    "                    'recall':      torchmetrics.Recall(task=\"binary\"),\n",
    "                    'f1':          torchmetrics.F1Score(task=\"binary\"),\n",
    "                    'auroc':       torchmetrics.AUROC(task=\"binary\"),\n",
    "                    'specificity': torchmetrics.Specificity(task=\"binary\"),\n",
    "                    'mcc':         torchmetrics.MatthewsCorrCoef(task=\"binary\"),\n",
    "                    'logauc':      torchmetrics.LogAUC(task=\"binary\")\n",
    "                }\n",
    "            ).to(self.device)\n",
    "\n",
    "\n",
    "            # NOTE: configuring weight_decay > 0.0 in torch.optim effectively applies L2 regularization, hence the variable l2 is passed \n",
    "            if optimizer == 'Adam':\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate)   #NOTE: only applies L1 (LASSO) if l1 > 0.0\n",
    "\n",
    "            elif optimizer == 'AdamW':\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "\n",
    "            elif optimizer == 'RMS':\n",
    "                optim = torch.optim.SGD(self.parameters(),  lr=learning_rate,  weight_decay=l2, momentum=momentum)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"@optimizer should be either 'Adam' (ADAM optimizer), 'AdamW' (idem. weight decay) or 'RMS' for RMSProp\")\n",
    "\n",
    "            for _ in range(num_epochs):\n",
    "                \n",
    "                train_loss = self.train_epoch(dataloader=train_dataloader, loss_fun=loss_fun, optim=optim, l1=l1, l2=l2)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                if validation_dataloader is not None:\n",
    "                    \n",
    "                    # Reset all metrics at the start of validation\n",
    "                    val_metrics.reset()\n",
    "                    \n",
    "                    metric_list = list(val_metrics.values())\n",
    "                    \n",
    "                    validation_loss = self.test(dataloader=validation_dataloader, loss_fun=loss_fun, metrics=metric_list)\n",
    "                    validation_losses.append(validation_loss)\n",
    "\n",
    "                    computed_metrics = val_metrics.compute()\n",
    "                    \n",
    "                    for k, v in computed_metrics.items():\n",
    "                        metrics_on_validation_data[k].append(v.item())\n",
    "\n",
    "            return train_losses, validation_losses, metrics_on_validation_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def grid_search_cv(training_dataset, \n",
    "                   param_dict, \n",
    "                   loss_factory: callable, \n",
    "                   k_folds=5, \n",
    "                   num_epochs=20, \n",
    "                   batch_size=32, \n",
    "                   device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Cross Validation over a grid of hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0. get input dimension \n",
    "    INPUT_DIM = training_dataset.tensors[0].shape[1]\n",
    "\n",
    "    # 1. Create the grid of all hyperparameter combinations\n",
    "    grid = list(\n",
    "            ParameterGrid(param_dict)\n",
    "        )\n",
    "    \n",
    "    # 2. Setup K-Fold splitter (Shuffle=True is important!)\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Outer Loop: Iterate over hyperparameter combinations\n",
    "    print(f\"Starting Grid Search with {len(grid)} candidates...\")\n",
    "    \n",
    "    for params in tqdm(grid, desc=\"Grid Search\"):\n",
    "        \n",
    "        fold_scores = []\n",
    "        \n",
    "        # Inner Loop: Cross Validation\n",
    "        # kfold.split yields indices directly, no need for manual math\n",
    "        for _, (train_idx, val_idx) in enumerate(kfold.split(training_dataset)):\n",
    "            \n",
    "            # Create Subsets and Loaders\n",
    "            train_sub = Subset(training_dataset, train_idx)\n",
    "            val_sub   = Subset(training_dataset, val_idx)\n",
    "            \n",
    "            train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "            val_loader   = DataLoader(val_sub,   batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # TODO: read input_dim dynamically \n",
    "            model = BinaryLogisticRegression(input_dim=INPUT_DIM)\n",
    "            model.to(device)\n",
    "            \n",
    "            # Create a fresh loss function (in case it holds state)\n",
    "            loss_fn = loss_factory()\n",
    "\n",
    "            _, _, metrics = model.train_model(\n",
    "                train_dataloader=train_loader,\n",
    "                validation_dataloader=val_loader,\n",
    "                loss_fun=loss_fn,\n",
    "                num_epochs=num_epochs,\n",
    "                **params \n",
    "            )\n",
    "            \n",
    "            # Store the metric of interest (e.g., PR-AUC from the last epoch)\n",
    "            # You might want to take max() instead of last index [-1] depending on stability\n",
    "            fold_scores.append(metrics['prauc'][-1])\n",
    "        \n",
    "        # Average score across folds\n",
    "        avg_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        \n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'avg_score': avg_score,\n",
    "            'std_score': std_score\n",
    "        })\n",
    "    \n",
    "    # TODO: update \n",
    "    results.sort(key=lambda x: x['avg_score'], reverse=True)\n",
    "    \n",
    "    best_result = results[0]\n",
    "    return best_result['params'], best_result['avg_score'], results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Starting Grid Search with 8 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   0%|          | 0/8 [00:00<?, ?it/s]/Users/spektralanalys/Desktop/torch_logistic_regression/.venv/lib/python3.12/site-packages/torchmetrics/functional/classification/logauc.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bounds = torch.log10(torch.tensor(fpr_range))\n",
      "Grid Search: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINNER PARAMS: {'l1': 0.01, 'l2': 0.0, 'learning_rate': 0.01}\n",
      "WINNER PR-AUC: 0.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: load data from .xlsx to X_training, y_training, X_test and y_test <'numpy.ndarray'>\n",
    "#X_tensor_training = torch.tensor(X_training, dtype=torch.float32)\n",
    "#y_tensor_training = torch.tensor(y_training, dtype=torch.float32)\n",
    "\n",
    "#training_tensor_dataset = torch.utils.data.TensorDataset(X_tensor_training, y_tensor_training)\n",
    "\n",
    "#X_tensor_testing = torch.tensor(X_testing, dtype=torch.float32)\n",
    "#y_tensor_testing = torch.tensor(y_testing, dtype=torch.float32)\n",
    "\n",
    "#testing_tensor_dataset = torch.utils.data.TensorDataset(X_tensor_testing, y_tensor_testing)\n",
    "\n",
    "\n",
    "## DUMMY DATA \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_DIM = 20\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=INPUT_DIM, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "X = numpy.array(X)\n",
    "y = numpy.array(y)\n",
    "\n",
    "\n",
    "# TODO: replace this with respect to time\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  \n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# use: X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "# y not scaled due to binary 1/0 \n",
    "\n",
    "print(type(X), type(y))\n",
    "\n",
    "training_tensor_dataset_scaled = torch.utils.data.TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "\n",
    "\n",
    "# B. Define Hyperparameter Grid\n",
    "parameter_grid = {\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'l1': [0.0, 0.01],\n",
    "    'l2': [0.0, 0.01]\n",
    "}\n",
    "    \n",
    "# TODO: import from log_reg_bin_loss_factories\n",
    "# DiceLoss, FocalLoss, JaccardLoss, OptimizedAsymmetricFocalLoss, TverskyBCELoss, TverskyLoss\n",
    "from log_reg_bin_loss_factories.TverskyBCELoss import *\n",
    "loss_factory = ComboLoss\n",
    "\n",
    "# D. Run Grid Search\n",
    "optimum_parameters, result_optimum_parameters, result_grid = grid_search_cv(training_tensor_dataset_scaled, param_dict=parameter_grid, loss_factory=loss_factory)\n",
    "\n",
    "print(f\"WINNER PARAMS: {optimum_parameters}\")\n",
    "print(f\"WINNER PR-AUC: {result_optimum_parameters:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POLARS\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "# Example Polars DF\n",
    "df = pl.read_excel(\"your_file.xlsx\")  # or pl.read_csv(), etc.\n",
    "\n",
    "# Separate features and target (adjust column names)\n",
    "X_pl = df.select(pl.col(\"^feature.*$\"))  # or explicit list of feature columns\n",
    "y_pl = df.select(\"target\")\n",
    "\n",
    "# Direct to torch tensor\n",
    "X_tensor = X_pl.to_torch()          # Shape: (n_samples, n_features), dtype=torch.float32 if f64\n",
    "y_tensor = y_pl.to_torch().float()  # Ensure float32 for labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
