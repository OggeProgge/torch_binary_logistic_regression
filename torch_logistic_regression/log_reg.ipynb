{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy  # NOTE: install v1.26.4\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_torch_f32 = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(in_features=input_dim, out_features=output_dim, bias=True)\n",
    "        self.metrics = {'accuracy': torchmetrics.Accuracy(task=\"binary\")}\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x).squeeze(-1)  # logits; squeeze() makes [x, 1] -> [x] i.e., just formatting \n",
    "\n",
    "\n",
    "    def reg_loss(self, l1: float) -> torch.Tensor:\n",
    "        \n",
    "        loss = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        if l1 > 0.0:\n",
    "            loss += l1 * torch.sum(abs(self.linear.weight)) \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # inference\n",
    "    def compute_prob(self, x) -> numpy.ndarray:\n",
    "        self.eval()\n",
    "\n",
    "        if isinstance(x, numpy.ndarray):\n",
    "            x = torch.from_numpy(x).float()\n",
    "\n",
    "        elif not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Instance must have type: 1) numpy.ndarray or 2) torch.Tensor\")\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            logits = self(x.to(self.device))\n",
    "            \n",
    "        return torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x, cl_threshold: float = 0.5) -> numpy.ndarray:\n",
    "\n",
    "        return (self.compute_prob(x) > cl_threshold).astype(int)\n",
    "    \n",
    "\n",
    "    # test\n",
    "    def test(self, dataloader: torch.utils.data.DataLoader, loss_fun, metrics=None):\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = []\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for X, y in dataloader:\n",
    "                \n",
    "                X, y = X.to(self.device, dtype=dtype_torch_f32), y.to(self.device, dtype=dtype_torch_f32)\n",
    "                \n",
    "                prediction = self(X)\n",
    "                \n",
    "                loss += loss_fun(prediction, y).item() * len(X)\n",
    "                \n",
    "                for m in metrics:\n",
    "                    m.update(torch.sigmoid(prediction), y.int())        \n",
    "\n",
    "        return loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "    # training\n",
    "    def train_epoch(self, dataloader: torch.utils.data.DataLoader, optim: torch.optim.Optimizer, loss_fun: callable, l1: float):\n",
    "\n",
    "        # train mode\n",
    "        self.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for _, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(self.device, dtype=dtype_torch_f32), y.to(self.device, dtype=dtype_torch_f32)    \n",
    "\n",
    "            # reset gradients\n",
    "            optim.zero_grad()                      \n",
    "\n",
    "            # prediction error\n",
    "            loss  = loss_fun(self(X), y)\n",
    "            loss += self.reg_loss(l1)\n",
    "\n",
    "            # training loss\n",
    "            train_loss += loss.item() * len(X)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        return train_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "    def train_model(self, \n",
    "                    train_dataloader: torch.utils.data.DataLoader, \n",
    "                    validation_dataloader: torch.utils.data.DataLoader,\n",
    "                    learning_rate: float, \n",
    "                    l1: float = 0.0,\n",
    "                    l2: float = 0.0,\n",
    "                    momentum: float = 0.9,\n",
    "                    loss_fun: callable = torch.nn.BCEWithLogitsLoss(),\n",
    "                    optimizer: str = \"Adam\",\n",
    "                    num_epochs: int = 100):\n",
    "\n",
    "            train_losses = []\n",
    "            validation_losses = []\n",
    "            \n",
    "            # Initialize dictionary to store results\n",
    "            metrics_on_validation_data = {k: [] for k in ['accuracy', 'precision', 'prauc', 'recall', 'f1', 'auroc', 'specificity', 'mcc', 'logauc']}\n",
    "\n",
    "            # Group metrics into a ModuleList or a standard list for easier management\n",
    "            # Note: torchmetrics.MetricCollection is designed exactly for this purpose!\n",
    "            val_metrics = torchmetrics.MetricCollection(\n",
    "                {\n",
    "                    'accuracy':    torchmetrics.Accuracy(task=\"binary\"),\n",
    "                    'precision':   torchmetrics.Precision(task=\"binary\"),\n",
    "                    'prauc':       torchmetrics.AveragePrecision(task=\"binary\"),\n",
    "                    'recall':      torchmetrics.Recall(task=\"binary\"),\n",
    "                    'f1':          torchmetrics.F1Score(task=\"binary\"),\n",
    "                    'auroc':       torchmetrics.AUROC(task=\"binary\"),\n",
    "                    'specificity': torchmetrics.Specificity(task=\"binary\"),\n",
    "                    'mcc':         torchmetrics.MatthewsCorrCoef(task=\"binary\"),\n",
    "                    'logauc':      torchmetrics.LogAUC(task=\"binary\")\n",
    "                }\n",
    "            ).to(self.device)\n",
    "\n",
    "\n",
    "            # NOTE: configuring weight_decay > 0.0 in torch.optim effectively applies L2 regularization, hence the variable l2 is passed \n",
    "            if optimizer == 'Adam':\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate)   #NOTE: only applies L1 (LASSO) if l1 > 0.0\n",
    "\n",
    "            elif optimizer == 'AdamW':\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "\n",
    "            elif optimizer == 'RMS':\n",
    "                optim = torch.optim.SGD(self.parameters(),  lr=learning_rate,  weight_decay=l2, momentum=momentum)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"@optimizer should be either 'Adam' (ADAM optimizer), 'AdamW' (idem. weight decay) or 'RMS' for RMSProp\")\n",
    "\n",
    "            for _ in range(num_epochs):\n",
    "                \n",
    "                train_loss = self.train_epoch(dataloader=train_dataloader, loss_fun=loss_fun, optim=optim, l1=l1, l2=l2)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                if validation_dataloader is not None:\n",
    "                    \n",
    "                    # Reset all metrics at the start of validation\n",
    "                    val_metrics.reset()\n",
    "                    \n",
    "                    metric_list = list(val_metrics.values())\n",
    "                    \n",
    "                    validation_loss = self.test(dataloader=validation_dataloader, loss_fun=loss_fun, metrics=metric_list)\n",
    "                    validation_losses.append(validation_loss)\n",
    "\n",
    "                    computed_metrics = val_metrics.compute()\n",
    "                    \n",
    "                    for k, v in computed_metrics.items():\n",
    "                        metrics_on_validation_data[k].append(v.item())\n",
    "\n",
    "            return train_losses, validation_losses, metrics_on_validation_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def grid_search_cv(training_dataset, \n",
    "                   param_dict, \n",
    "                   loss_factory: callable, \n",
    "                   k_folds=5, \n",
    "                   num_epochs=20, \n",
    "                   batch_size=32, \n",
    "                   device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Cross Validation over a grid of hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0. get input dimension \n",
    "    INPUT_DIM = training_dataset.tensors[0].shape[1]\n",
    "\n",
    "    # 1. Create the grid of all hyperparameter combinations\n",
    "    grid = list(\n",
    "            ParameterGrid(param_dict)\n",
    "        )\n",
    "    \n",
    "    # 2. Setup K-Fold splitter (Shuffle=True is important!)\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Outer Loop: Iterate over hyperparameter combinations\n",
    "    print(f\"Starting Grid Search with {len(grid)} candidates...\")\n",
    "    \n",
    "    for params in tqdm(grid, desc=\"Grid Search\"):\n",
    "        \n",
    "        fold_scores = []\n",
    "        \n",
    "        # Inner Loop: Cross Validation\n",
    "        # kfold.split yields indices directly, no need for manual math\n",
    "        for _, (train_idx, val_idx) in enumerate(kfold.split(training_dataset)):\n",
    "            \n",
    "            # Create Subsets and Loaders\n",
    "            train_sub = Subset(training_dataset, train_idx)\n",
    "            val_sub   = Subset(training_dataset, val_idx)\n",
    "            \n",
    "            train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "            val_loader   = DataLoader(val_sub,   batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # TODO: read input_dim dynamically \n",
    "            model = BinaryLogisticRegression(input_dim=INPUT_DIM)\n",
    "            model.to(device)\n",
    "            \n",
    "            # Create a fresh loss function (in case it holds state)\n",
    "            loss_fn = loss_factory()\n",
    "\n",
    "            _, _, metrics = model.train_model(\n",
    "                train_dataloader=train_loader,\n",
    "                validation_dataloader=val_loader,\n",
    "                loss_fun=loss_fn,\n",
    "                num_epochs=num_epochs,\n",
    "                **params \n",
    "            )\n",
    "            \n",
    "            # Store the metric of interest (e.g., PR-AUC from the last epoch)\n",
    "            # You might want to take max() instead of last index [-1] depending on stability\n",
    "            fold_scores.append(metrics['prauc'][-1])\n",
    "        \n",
    "        # Average score across folds\n",
    "        avg_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        \n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'avg_score': avg_score,\n",
    "            'std_score': std_score\n",
    "        })\n",
    "    \n",
    "    # TODO: update \n",
    "    results.sort(key=lambda x: x['avg_score'], reverse=True)\n",
    "    \n",
    "    best_result = results[0]\n",
    "    return best_result['params'], best_result['avg_score'], results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Starting Grid Search with 8 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   0%|          | 0/8 [00:00<?, ?it/s]/Users/spektralanalys/Desktop/torch_logistic_regression/.venv/lib/python3.12/site-packages/torchmetrics/functional/classification/logauc.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bounds = torch.log10(torch.tensor(fpr_range))\n",
      "Grid Search: 100%|██████████| 8/8 [00:14<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINNER PARAMS: {'l1': 0.01, 'l2': 0.0, 'learning_rate': 0.01}\n",
      "WINNER PR-AUC: 0.7110\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 64 and the array at index 3 has size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m     predictions.append(y_pred_proba)\n\u001b[32m     95\u001b[39m     true_labels.append(batch_y.cpu().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m y_pred_proba = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m y_true = np.vstack(true_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/torch_logistic_regression/.venv/lib/python3.12/site-packages/numpy/core/shape_base.py:289\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    288\u001b[39m     arrs = [arrs]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 64 and the array at index 3 has size 8"
     ]
    }
   ],
   "source": [
    "# TODO: load data from .xlsx to X_training, y_training, X_test and y_test <'numpy.ndarray'>\n",
    "#X_tensor_training = torch.tensor(X_training, dtype=torch.float32)\n",
    "#y_tensor_training = torch.tensor(y_training, dtype=torch.float32)\n",
    "\n",
    "#training_tensor_dataset = torch.utils.data.TensorDataset(X_tensor_training, y_tensor_training)\n",
    "\n",
    "#X_tensor_testing = torch.tensor(X_testing, dtype=torch.float32)\n",
    "#y_tensor_testing = torch.tensor(y_testing, dtype=torch.float32)\n",
    "\n",
    "#testing_tensor_dataset = torch.utils.data.TensorDataset(X_tensor_testing, y_tensor_testing)\n",
    "\n",
    "\n",
    "## DUMMY DATA \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_DIM = 20\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=INPUT_DIM, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "X = numpy.array(X)\n",
    "y = numpy.array(y)\n",
    "\n",
    "\n",
    "# TODO: replace this with respect to time\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  \n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "# y not scaled due to binary 1/0 \n",
    "\n",
    "print(type(X), type(y))\n",
    "\n",
    "train_tensor_dataset_scaled = torch.utils.data.TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "\n",
    "# B. Define Hyperparameter Grid\n",
    "parameter_grid = {\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'l1': [0.0, 0.01],\n",
    "    'l2': [0.0, 0.01]\n",
    "}\n",
    "    \n",
    "# TODO: import from log_reg_bin_loss_factories\n",
    "# DiceLoss, FocalLoss, JaccardLoss, OptimizedAsymmetricFocalLoss, TverskyBCELoss, TverskyLoss\n",
    "from log_reg_bin_loss_factories.TverskyBCELoss import *\n",
    "loss_factory = ComboLoss\n",
    "\n",
    "# D. Run Grid Search\n",
    "optimum_parameters, result_optimum_parameters, result_grid = grid_search_cv(train_tensor_dataset_scaled, param_dict=parameter_grid, loss_factory=loss_factory)\n",
    "\n",
    "print(f\"WINNER PARAMS: {optimum_parameters}\")\n",
    "print(f\"WINNER PR-AUC: {result_optimum_parameters:.4f}\")\n",
    "\n",
    "\n",
    "# train final model\n",
    "\n",
    "model = BinaryLogisticRegression(input_dim=INPUT_DIM)\n",
    "model.to(\"cpu\")\n",
    "            \n",
    "# Create a fresh loss function (in case it holds state)\n",
    "\n",
    "train_loader = DataLoader(train_tensor_dataset_scaled, batch_size=32, shuffle=False)\n",
    "\n",
    "train_losses, _, _ = model.train_model(\n",
    "                            train_dataloader=train_loader,\n",
    "                            validation_dataloader=None,\n",
    "                            loss_fun=loss_factory(),\n",
    "                            num_epochs=20,\n",
    "                            **optimum_parameters \n",
    "                        )   \n",
    "\n",
    "\n",
    "# inference using final model: \n",
    "model.eval()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch_x, batch_y in test_loader:\n",
    "\n",
    "        y_pred_proba = torch.sigmoid(model(batch_x)).squeeze().cpu().numpy()\n",
    "\n",
    "        predictions.append(y_pred_proba)\n",
    "        true_labels.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4.64510992e-02, 1.90900728e-01, 1.66361604e-03, 1.15280606e-01,\n",
       "        4.29387391e-01, 1.21278968e-02, 8.59693229e-01, 6.74770921e-02,\n",
       "        7.34421331e-03, 5.82810044e-01, 1.01189045e-02, 2.55899858e-02,\n",
       "        1.67615674e-02, 5.24447188e-02, 4.37844098e-01, 8.48752782e-02,\n",
       "        3.33504558e-01, 5.52346371e-02, 6.91705644e-02, 6.56111956e-01,\n",
       "        1.00427337e-01, 3.45393978e-02, 7.04404190e-02, 4.36951071e-02,\n",
       "        3.79726320e-04, 5.22701144e-02, 4.45567956e-03, 2.80553233e-02,\n",
       "        1.23283215e-01, 7.08457977e-02, 7.45066047e-01, 4.39354703e-02,\n",
       "        5.06197624e-02, 9.23535228e-01, 1.82726514e-02, 1.61602627e-02,\n",
       "        6.45637214e-01, 7.92777017e-02, 1.17289321e-02, 3.74305755e-01,\n",
       "        1.19972229e-01, 1.28955022e-02, 4.49498184e-04, 2.84874579e-03,\n",
       "        4.17834558e-02, 7.03931630e-01, 3.47011179e-01, 1.04782879e-01,\n",
       "        6.08571507e-02, 3.70521545e-02, 3.11058182e-02, 1.44196209e-02,\n",
       "        9.58521143e-02, 7.96412900e-02, 1.14359371e-02, 6.78423047e-02,\n",
       "        5.80542833e-02, 2.70064808e-02, 7.15363864e-03, 3.68626751e-02,\n",
       "        1.14248111e-03, 5.67673109e-02, 1.00325532e-02, 7.29628503e-02],\n",
       "       dtype=float32),\n",
       " array([4.1422494e-02, 8.1370676e-01, 1.4710039e-01, 4.0786299e-01,\n",
       "        6.8496102e-01, 8.3325118e-02, 4.1033868e-03, 3.0113028e-02,\n",
       "        1.5749006e-02, 6.7710392e-02, 2.5898214e-02, 5.9405461e-02,\n",
       "        8.3057173e-02, 3.3032507e-02, 1.8882015e-04, 6.5118164e-01,\n",
       "        2.3832161e-02, 6.3479908e-02, 1.0940191e-02, 6.8227403e-02,\n",
       "        7.6912212e-01, 7.1522260e-01, 1.9431493e-01, 4.7468521e-02,\n",
       "        2.3151061e-01, 5.1889326e-02, 1.1984530e-02, 2.4325727e-03,\n",
       "        7.5586893e-02, 6.0233086e-02, 9.7326539e-02, 1.0903688e-01,\n",
       "        2.3616338e-01, 2.9571214e-01, 4.0952925e-02, 5.0201777e-02,\n",
       "        5.2935723e-02, 2.2358559e-02, 2.8331095e-02, 5.4488696e-02,\n",
       "        2.5989193e-02, 9.9116653e-01, 7.6198289e-03, 7.0228510e-02,\n",
       "        1.1567106e-01, 4.5702275e-02, 2.2971926e-03, 2.5654968e-02,\n",
       "        1.7744295e-01, 1.3196691e-02, 1.1188183e-01, 6.5795019e-02,\n",
       "        3.4970030e-01, 6.0499752e-01, 6.8386114e-01, 4.9020860e-02,\n",
       "        1.3810119e-01, 3.2833572e-02, 1.6228279e-02, 9.6607214e-01,\n",
       "        6.4589389e-02, 3.7006084e-02, 5.5816188e-02, 2.3526239e-01],\n",
       "       dtype=float32),\n",
       " array([2.0782536e-01, 1.2742342e-01, 3.8593888e-02, 4.1810125e-02,\n",
       "        8.8627394e-03, 4.5757148e-02, 1.0155283e-02, 4.1781824e-02,\n",
       "        4.9208407e-03, 3.6251871e-03, 1.0048033e-01, 5.9889799e-01,\n",
       "        4.4635497e-02, 1.0135601e-02, 2.4090329e-02, 4.7000948e-02,\n",
       "        2.4510685e-03, 7.8322791e-04, 4.9159221e-02, 1.0003833e-03,\n",
       "        8.9559174e-04, 1.0644180e-01, 4.0098629e-03, 1.5605956e-01,\n",
       "        4.9828812e-02, 2.9876322e-01, 9.1142213e-01, 3.4621380e-02,\n",
       "        1.1515734e-01, 6.9115229e-02, 8.6893244e-03, 2.0107004e-01,\n",
       "        5.9480304e-01, 1.1885249e-02, 7.3569193e-02, 3.7916679e-02,\n",
       "        2.4552168e-03, 2.0696612e-01, 3.1343028e-01, 1.6935399e-02,\n",
       "        4.5662113e-03, 1.2998536e-01, 2.9478738e-01, 7.4958946e-03,\n",
       "        2.3031780e-02, 9.2816234e-01, 7.9822861e-02, 2.5651339e-02,\n",
       "        1.0112632e-01, 1.5878322e-02, 7.1444586e-02, 4.3983951e-02,\n",
       "        1.4979346e-02, 7.9938201e-03, 8.3977008e-01, 9.7367847e-03,\n",
       "        3.3206359e-02, 3.1333487e-02, 9.6388767e-03, 5.9756719e-02,\n",
       "        1.2692198e-02, 1.5057760e-04, 2.0924811e-01, 4.5936566e-04],\n",
       "       dtype=float32),\n",
       " array([0.17374021, 0.00260966, 0.07550339, 0.11492319, 0.43898463,\n",
       "        0.02276493, 0.15474613, 0.06401832], dtype=float32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32),\n",
       " array([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
